{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LR_BERT_Sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88AgSPuLKOY",
        "colab_type": "text"
      },
      "source": [
        "The model used in this notebook is inspired from :\n",
        "Duyu Tang, Bing Qin, Xiaocheng Feng, Ting Liu. 2016. 'Effective LSTMs for Target-Dependent Sentiment Classification'. https://arxiv.org/abs/1512.01100\n",
        "\n",
        "In the above mentioned paper, they developed two target dependent long short-term memory (LSTM) models shown in Figure 1. Instead of LSTMs, BERT model is used in this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-9r8CSnNQPg",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/file/d/1HnS2uZhalTo7T7f_K6yqEzSm9XU7lFwg/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UYJGiACI27P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "2651d37b-155b-44a3-e7d2-1ee861e1b74a"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.13.23)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.23 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.16.23)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.23->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.23->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.23->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CHIPWSrCQAvs",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string \n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Precision, Recall, FalseNegatives, FalsePositives\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from tensorflow.keras.models import Model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GF0p7uiHQnkD"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uMT5swFUQnkg",
        "colab": {}
      },
      "source": [
        "# Import data\n",
        "train = pd.read_excel('Trainset.xlsx')\n",
        "test = pd.read_excel('Testset.xlsx')\n",
        "\n",
        "# Eliminate the NAs\n",
        "train = train.fillna('')\n",
        "test = test.fillna('')\n",
        "\n",
        "# Remove the rows without Opinion Category values\n",
        "train = train[train.OpinionCategory != ''] \n",
        "test = test[test.OpinionCategory != ''] \n",
        "\n",
        "# Sort the data\n",
        "train = train.sort_values('ID_and_Review').reset_index(drop=True)\n",
        "test = test.sort_values('ID_and_Review').reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR63UypfPMmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "2ee3d30f-c91c-45a3-d0bd-dac2d8e61297"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_number</th>\n",
              "      <th>Review_ID</th>\n",
              "      <th>ID_and_Review</th>\n",
              "      <th>OutOfScope</th>\n",
              "      <th>Sentence_ID</th>\n",
              "      <th>OpinionCategory</th>\n",
              "      <th>OpinionFrom</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>AspectTerm</th>\n",
              "      <th>OpinionTo</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:0</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>RESTAURANT#GENERAL</td>\n",
              "      <td>51</td>\n",
              "      <td>negative</td>\n",
              "      <td>place</td>\n",
              "      <td>56</td>\n",
              "      <td>Judging from previous posts this used to be a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:1</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>75</td>\n",
              "      <td>negative</td>\n",
              "      <td>staff</td>\n",
              "      <td>80</td>\n",
              "      <td>We, there were four of us, arrived at noon - t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:2</td>\n",
              "      <td></td>\n",
              "      <td>3</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>They never brought us complimentary noodles, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:3</td>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>4</td>\n",
              "      <td>negative</td>\n",
              "      <td>food</td>\n",
              "      <td>8</td>\n",
              "      <td>The food was lousy - too sweet or too salty an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:3</td>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "      <td>FOOD#STYLE_OPTIONS</td>\n",
              "      <td>52</td>\n",
              "      <td>negative</td>\n",
              "      <td>portions</td>\n",
              "      <td>60</td>\n",
              "      <td>The food was lousy - too sweet or too salty an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID_number  ...                                               Text\n",
              "0   1004293  ...  Judging from previous posts this used to be a ...\n",
              "1   1004293  ...  We, there were four of us, arrived at noon - t...\n",
              "2   1004293  ...  They never brought us complimentary noodles, i...\n",
              "3   1004293  ...  The food was lousy - too sweet or too salty an...\n",
              "4   1004293  ...  The food was lousy - too sweet or too salty an...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j0BqJOHLQnkt",
        "outputId": "383e227c-9aeb-4bfa-bab5-b30a239200a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train.Polarity.value_counts(), test.Polarity.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(positive    1657\n",
              " negative     749\n",
              " neutral      101\n",
              " Name: Polarity, dtype: int64, positive    611\n",
              " negative    204\n",
              " neutral      44\n",
              " Name: Polarity, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S-qK5pCPQuZ",
        "colab_type": "text"
      },
      "source": [
        "Train data consists of 11 variables. The four of them indicate the ID numbers of the sentences, the reviewer, the review and the combination of them. OutofScope variable loses its function when I eliminated the null OpinionCategory values. The Opinion Category shows the aspect which the review refers to. The Opinion Category consists of 12 classes and each class has an entity and a corresponding attribute, in other words, E#A pairs. \n",
        "\n",
        "In this notebook, I will deal only with the Polarity and the corresponding reviews under the Text column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hS4vLNQBQnkw"
      },
      "source": [
        "### y_train / y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PJZ6DpmPQnk1",
        "colab": {}
      },
      "source": [
        "y_train = to_categorical(train.Polarity.astype('category').cat.codes)\n",
        "y_test = to_categorical(test.Polarity.astype('category').cat.codes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4uUfHAS-Qnk3",
        "outputId": "10f2dae6-c2d3-4f37-a95a-1d8a30494de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "y_train, y_train.shape, y_test, y_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 0., 1.]], dtype=float32), (2507, 3), array([[0., 0., 1.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 1., 0.],\n",
              "        ...,\n",
              "        [0., 0., 1.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 0., 1.]], dtype=float32), (859, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcAnSfg7QnlF"
      },
      "source": [
        "## Data Processing for the BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHceqQ5AQsIV",
        "colab_type": "text"
      },
      "source": [
        "### beforeAspect List & afterAspect List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyJ6j6QiQ0du",
        "colab_type": "text"
      },
      "source": [
        "To be able to create two target dependent BERT models, we need to divide the text in two parts. First part captures the beginning of the sentence, including the aspect term as the last word, while the second part captures the ending of the sentence which begins with the aspect term of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qNF5kqB2Dn5c",
        "colab": {}
      },
      "source": [
        "beforeAspectList = [] \n",
        "for i in range(len(train.Text)):\n",
        "    opinionTo = int(train.OpinionTo[i])\n",
        "    if (opinionTo == 0):\n",
        "        beforeAspect = []\n",
        "        beforeAspectList.append(beforeAspect)\n",
        "    else:\n",
        "        beforeAspect = train.Text[i][0:opinionTo]\n",
        "        beforeAspectList.append(beforeAspect)\n",
        "        \n",
        "beforeAspectList_test = [] \n",
        "for i in range(len(test.Text)):\n",
        "    opinionTo = int(test.OpinionTo[i])\n",
        "    if (opinionTo == 0):\n",
        "        beforeAspect = []\n",
        "        beforeAspectList_test.append(beforeAspect)\n",
        "    else:\n",
        "        beforeAspect = test.Text[i][0:opinionTo]\n",
        "        beforeAspectList_test.append(beforeAspect)\n",
        "        \n",
        "afterAspectList = [] \n",
        "\n",
        "for i in range(len(train.Text)):\n",
        "    OpinionFrom = int(train.OpinionFrom[i])\n",
        "    if (OpinionFrom == 0):\n",
        "        afterAspect = train.Text[i]\n",
        "        afterAspectList.append(afterAspect)\n",
        "    else:\n",
        "        afterAspect = train.Text[i][OpinionFrom:len(train.Text[i])]\n",
        "        afterAspectList.append(afterAspect)\n",
        "        \n",
        "afterAspectList_test = [] \n",
        "\n",
        "for i in range(len(test.Text)):\n",
        "    OpinionFrom = int(test.OpinionFrom[i])\n",
        "    if (OpinionFrom == 0):\n",
        "        afterAspect = test.Text[i]\n",
        "        afterAspectList_test.append(afterAspect)\n",
        "    else:\n",
        "        afterAspect = test.Text[i][OpinionFrom:len(test.Text[i])]\n",
        "        afterAspectList_test.append(afterAspect)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7OeCFa7Dn5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "062a90c3-6a69-4fa5-e62f-b2d0a2e00918"
      },
      "source": [
        "len(beforeAspectList), len(beforeAspectList_test), len(afterAspectList), len(afterAspectList_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2507, 859, 2507, 859)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XRsy8MAQnlI",
        "colab": {}
      },
      "source": [
        "train.insert(9, \"beforeAspect\", pd.Series(beforeAspectList).astype(str))\n",
        "train.insert(11, \"afterAspect\", pd.Series(afterAspectList).astype(str))\n",
        "\n",
        "test.insert(9, \"beforeAspect\", pd.Series(beforeAspectList_test).astype(str))\n",
        "test.insert(11, \"afterAspect\", pd.Series(afterAspectList_test).astype(str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zGnJP-0sQnlJ",
        "outputId": "f5e73dd3-e4cb-4286-dcb3-f847b95d2dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_number</th>\n",
              "      <th>Review_ID</th>\n",
              "      <th>ID_and_Review</th>\n",
              "      <th>OutOfScope</th>\n",
              "      <th>Sentence_ID</th>\n",
              "      <th>OpinionCategory</th>\n",
              "      <th>OpinionFrom</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>AspectTerm</th>\n",
              "      <th>beforeAspect</th>\n",
              "      <th>OpinionTo</th>\n",
              "      <th>afterAspect</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:0</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>RESTAURANT#GENERAL</td>\n",
              "      <td>51</td>\n",
              "      <td>negative</td>\n",
              "      <td>place</td>\n",
              "      <td>Judging from previous posts this used to be a ...</td>\n",
              "      <td>56</td>\n",
              "      <td>place, but not any longer.</td>\n",
              "      <td>Judging from previous posts this used to be a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:1</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>75</td>\n",
              "      <td>negative</td>\n",
              "      <td>staff</td>\n",
              "      <td>We, there were four of us, arrived at noon - t...</td>\n",
              "      <td>80</td>\n",
              "      <td>staff acted like we were imposing on them and ...</td>\n",
              "      <td>We, there were four of us, arrived at noon - t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:2</td>\n",
              "      <td></td>\n",
              "      <td>3</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>They never brought us complimentary noodles, i...</td>\n",
              "      <td>They never brought us complimentary noodles, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:3</td>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>4</td>\n",
              "      <td>negative</td>\n",
              "      <td>food</td>\n",
              "      <td>The food</td>\n",
              "      <td>8</td>\n",
              "      <td>food was lousy - too sweet or too salty and th...</td>\n",
              "      <td>The food was lousy - too sweet or too salty an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1004293</td>\n",
              "      <td>1</td>\n",
              "      <td>1004293:3</td>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "      <td>FOOD#STYLE_OPTIONS</td>\n",
              "      <td>52</td>\n",
              "      <td>negative</td>\n",
              "      <td>portions</td>\n",
              "      <td>The food was lousy - too sweet or too salty an...</td>\n",
              "      <td>60</td>\n",
              "      <td>portions tiny.</td>\n",
              "      <td>The food was lousy - too sweet or too salty an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID_number  ...                                               Text\n",
              "0   1004293  ...  Judging from previous posts this used to be a ...\n",
              "1   1004293  ...  We, there were four of us, arrived at noon - t...\n",
              "2   1004293  ...  They never brought us complimentary noodles, i...\n",
              "3   1004293  ...  The food was lousy - too sweet or too salty an...\n",
              "4   1004293  ...  The food was lousy - too sweet or too salty an...\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__aBzOT3R4sg",
        "colab_type": "text"
      },
      "source": [
        "### BERT Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tPnOesW3QnlM",
        "outputId": "ed742716-db16-4eab-f223-9c923389eb9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# add special tokens for BERT to work properly\n",
        "sentences_before = [\"[CLS] \" + sent + \" [SEP]\" for sent in train.beforeAspect]\n",
        "sentences_before_test = [\"[CLS] \" + sent + \" [SEP]\" for sent in test.beforeAspect]\n",
        "\n",
        "sentences_after = [\"[CLS] \" + sent + \" [SEP]\" for sent in train.afterAspect]\n",
        "sentences_after_test = [\"[CLS] \" + sent + \" [SEP]\" for sent in test.afterAspect]\n",
        "\n",
        "sentences_before[0], sentences_before_test[0], sentences_after[0], sentences_after_test[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS] Judging from previous posts this used to be a good place [SEP]',\n",
              " '[CLS] [] [SEP]',\n",
              " '[CLS] place, but not any longer. [SEP]',\n",
              " '[CLS] Yum! [SEP]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFmQfcAMSCjY",
        "colab_type": "text"
      },
      "source": [
        "For the tokenization, pre-trained Bert-Base-Uncased dictionary is used. They constructed it with WordPiece embeddings with a 30,000 token vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZF-iJuhQnlO",
        "colab": {}
      },
      "source": [
        "# Tokenize with BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_before = [tokenizer.tokenize(sent) for sent in sentences_before]\n",
        "tokenized_before_test = [tokenizer.tokenize(sent) for sent in sentences_before_test]\n",
        "\n",
        "tokenized_after = [tokenizer.tokenize(sent) for sent in sentences_after]\n",
        "tokenized_after_test = [tokenizer.tokenize(sent) for sent in sentences_after_test]\n",
        "\n",
        "#tokenized_texts[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86CkL0rwSLjx",
        "colab_type": "text"
      },
      "source": [
        "For the BERT model to work, we need three inputs. \n",
        "- Input IDs: shows the ID number of each token with padding. The ID numbers are restored from the BERT vocabulary dictionary.\n",
        "- Mask IDs: indicates which elements in the sequence are tokens and which are padding elements.\n",
        "- Segment IDs: distinguishes different sentences, 0 for one-sentence sequence, 1 if there are two sentences.\n",
        "\n",
        "The functions below are extracted from: https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "caA-9DMLQnlQ",
        "colab": {}
      },
      "source": [
        "def get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn1tprYzSfQ_",
        "colab_type": "text"
      },
      "source": [
        "### Inputs for the before aspect BERT model (left BERT model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_qTQvlYyQnlS",
        "colab": {}
      },
      "source": [
        "# find the longest sequence for the padding\n",
        "def find_max_list(list):\n",
        "    list_len = [len(i) for i in list]\n",
        "    return max(list_len)\n",
        "    \n",
        "longestSeq_train = find_max_list(tokenized_before)\n",
        "longestSeq_test = find_max_list(tokenized_before_test)\n",
        "max_seq_length_before = max(longestSeq_train, longestSeq_test)\n",
        "\n",
        "# Find input_ids, mask_ids and segment_ids for the before aspect BERT model\n",
        "input_ids_before = []\n",
        "for i in range(len(tokenized_before)):\n",
        "    input_ids_before.append(get_ids(tokenized_before[i], tokenizer, max_seq_length_before))\n",
        "\n",
        "input_masks_before = [] \n",
        "for i in range(len(tokenized_before)):\n",
        "    input_masks_before.append(get_masks(tokenized_before[i], max_seq_length_before))\n",
        "    \n",
        "input_segments_before = [] \n",
        "for i in range(len(tokenized_before)):\n",
        "    input_segments_before.append(get_segments(tokenized_before[i], max_seq_length_before))\n",
        "    \n",
        "input_ids_before_test = []\n",
        "for i in range(len(tokenized_before_test)):\n",
        "    input_ids_before_test.append(get_ids(tokenized_before_test[i], tokenizer, max_seq_length_before))\n",
        "\n",
        "input_masks_before_test = [] \n",
        "for i in range(len(tokenized_before_test)):\n",
        "    input_masks_before_test.append(get_masks(tokenized_before_test[i], max_seq_length_before))\n",
        "    \n",
        "input_segments_before_test = [] \n",
        "for i in range(len(tokenized_before_test)):\n",
        "    input_segments_before_test.append(get_segments(tokenized_before_test[i], max_seq_length_before))\n",
        "    \n",
        "# For the model, I converted the lists to tensors.\n",
        "input_ids_before = tf.convert_to_tensor(input_ids_before, dtype = tf.int32)\n",
        "input_masks_before = tf.convert_to_tensor(input_masks_before, dtype = tf.int32)\n",
        "input_segments_before = tf.convert_to_tensor(input_segments_before, dtype = tf.int32)\n",
        "\n",
        "input_ids_before_test = tf.convert_to_tensor(input_ids_before_test, dtype = tf.int32)\n",
        "input_masks_before_test = tf.convert_to_tensor(input_masks_before_test, dtype = tf.int32)\n",
        "input_segments_before_test = tf.convert_to_tensor(input_segments_before_test, dtype = tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICpBLRwWS-kE",
        "colab_type": "text"
      },
      "source": [
        "### Inputs for the after aspect BERT model (right BERT model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I6Ojg-8HQnlU",
        "colab": {}
      },
      "source": [
        "# find the longest sequence for the padding\n",
        "def find_max_list(list):\n",
        "    list_len = [len(i) for i in list]\n",
        "    return max(list_len)\n",
        "    \n",
        "longestSeq_train = find_max_list(tokenized_after)\n",
        "longestSeq_test = find_max_list(tokenized_after_test)\n",
        "max_seq_length_after = max(longestSeq_train, longestSeq_test)\n",
        "\n",
        "# Find input_ids, mask_ids and segment_ids for the after aspect BERT model\n",
        "input_ids_after = []\n",
        "for i in range(len(tokenized_after)):\n",
        "    input_ids_after.append(get_ids(tokenized_after[i], tokenizer, max_seq_length_after))\n",
        "\n",
        "input_masks_after = [] \n",
        "for i in range(len(tokenized_after)):\n",
        "    input_masks_after.append(get_masks(tokenized_after[i], max_seq_length_after))\n",
        "    \n",
        "input_segments_after = [] \n",
        "for i in range(len(tokenized_after)):\n",
        "    input_segments_after.append(get_segments(tokenized_after[i], max_seq_length_after))\n",
        "    \n",
        "input_ids_after_test = []\n",
        "for i in range(len(tokenized_after_test)):\n",
        "    input_ids_after_test.append(get_ids(tokenized_after_test[i], tokenizer, max_seq_length_after))\n",
        "\n",
        "input_masks_after_test = [] \n",
        "for i in range(len(tokenized_after_test)):\n",
        "    input_masks_after_test.append(get_masks(tokenized_after_test[i], max_seq_length_after))\n",
        "    \n",
        "input_segments_after_test = [] \n",
        "for i in range(len(tokenized_after_test)):\n",
        "    input_segments_after_test.append(get_segments(tokenized_after_test[i], max_seq_length_after))\n",
        "\n",
        "# For the model, I converted the lists to tensors.    \n",
        "input_ids_after = tf.convert_to_tensor(input_ids_after, dtype = tf.int32)\n",
        "input_masks_after = tf.convert_to_tensor(input_masks_after, dtype = tf.int32)\n",
        "input_segments_after = tf.convert_to_tensor(input_segments_after, dtype = tf.int32)\n",
        "\n",
        "input_ids_after_test = tf.convert_to_tensor(input_ids_after_test, dtype = tf.int32)\n",
        "input_masks_after_test = tf.convert_to_tensor(input_masks_after_test, dtype = tf.int32)\n",
        "input_segments_after_test = tf.convert_to_tensor(input_segments_after_test, dtype = tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Iay5FCDWQnlW"
      },
      "source": [
        "## BERT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpVgVRkpQnlX",
        "outputId": "ac78b613-2aef-46cc-e006-a1a3e0533705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "random.seed(123)\n",
        "# Three Inputs of the Left Bert Model\n",
        "InputIDLayer_left = Input(shape=(max_seq_length_before,), dtype=tf.int32, name=\"InputIDs_left\")\n",
        "MaskIDLayer_left = Input(shape = (max_seq_length_before,), dtype = tf.int32, name = \"MaskIDs_left\")\n",
        "SegmentIDLayer_left = Input(shape = (max_seq_length_before,), dtype = tf.int32, name = \"SegmentIDs_left\")\n",
        "\n",
        "# Import the pre-trained uncased Bert model\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=True)\n",
        "\n",
        "# Since it is a classisfication problem, the pooled output is needed.\n",
        "PooledOutput_left, SequenceOutput_left = bert_layer([InputIDLayer_left, MaskIDLayer_left, SegmentIDLayer_left])\n",
        "output_left = Dense(258)(PooledOutput_left)\n",
        "\n",
        "# Three Inputs of the Right Bert Model\n",
        "InputIDLayer_right = Input(shape=(max_seq_length_after,), dtype=tf.int32, name=\"InputIDs_right\")\n",
        "MaskIDLayer_right = Input(shape = (max_seq_length_after,), dtype = tf.int32, name = \"MaskIDs_right\")\n",
        "SegmentIDLayer_right = Input(shape = (max_seq_length_after,), dtype = tf.int32, name = \"SegmentIDs_right\")\n",
        "\n",
        "# Since it is a classisfication problem, the pooled output is needed.\n",
        "PooledOutput_right, SequenceOutput_right = bert_layer([InputIDLayer_right, MaskIDLayer_right, SegmentIDLayer_right])\n",
        "output_right = Dense(258)(PooledOutput_right)\n",
        "\n",
        "# Concatenate the layers and classify with Dense\n",
        "allLayers = tf.keras.layers.concatenate([output_left, output_right])\n",
        "output = Dense(3, activation = 'sigmoid')(allLayers)\n",
        "\n",
        "model = Model(inputs=[InputIDLayer_left, MaskIDLayer_left, SegmentIDLayer_left,\n",
        "                     InputIDLayer_right, MaskIDLayer_right, SegmentIDLayer_right], outputs = [output])\n",
        "\n",
        "# Model Compilation\n",
        "learning_rate = 2e-5\n",
        "number_of_epochs = 10\n",
        "optimizer = Adam(learning_rate = learning_rate, epsilon = 1e-08)\n",
        "loss = CategoricalCrossentropy(from_logits = False)\n",
        "metrics = [Precision(), Recall(),\n",
        "          FalseNegatives(), FalsePositives()]\n",
        "\n",
        "model.compile(optimizer = optimizer, \n",
        "              loss = loss,\n",
        "              metrics = metrics)\n",
        "\n",
        "# Model Training & Fine-Tuning on train data\n",
        "earlyStopping = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 1)\n",
        "\n",
        "bert_history = model.fit([input_ids_before, input_masks_before, input_segments_before,\n",
        "                          input_ids_after, input_masks_after, input_segments_after], [y_train],\n",
        "                         epochs = number_of_epochs, \n",
        "                         batch_size = 32,\n",
        "                         validation_split = 0.1,\n",
        "                         callbacks = [earlyStopping]\n",
        "                         )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "71/71 [==============================] - 51s 712ms/step - loss: 0.6889 - precision: 0.6197 - recall: 0.8160 - false_negatives: 415.0000 - false_positives: 1130.0000 - val_loss: 0.5453 - val_precision: 0.7574 - val_recall: 0.6096 - val_false_negatives: 98.0000 - val_false_positives: 49.0000\n",
            "Epoch 2/10\n",
            "71/71 [==============================] - 47s 668ms/step - loss: 0.3430 - precision: 0.9342 - recall: 0.6414 - false_negatives: 809.0000 - false_positives: 102.0000 - val_loss: 0.5098 - val_precision: 0.9167 - val_recall: 0.4821 - val_false_negatives: 130.0000 - val_false_positives: 11.0000\n",
            "Epoch 3/10\n",
            "71/71 [==============================] - 47s 668ms/step - loss: 0.1863 - precision: 0.9876 - recall: 0.6356 - false_negatives: 822.0000 - false_positives: 18.0000 - val_loss: 0.5518 - val_precision: 0.9444 - val_recall: 0.4064 - val_false_negatives: 149.0000 - val_false_positives: 6.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3jSFc3rRanz3",
        "colab": {}
      },
      "source": [
        "# Predictions\n",
        "pred = model.predict([input_ids_before_test, input_masks_before_test, input_segments_before_test,\n",
        "                          input_ids_after_test, input_masks_after_test, input_segments_after_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ssvGNlVZbNEr",
        "outputId": "bd937c59-3cf2-4c23-ff01-09d65627535d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Model Evaluation - Loss, Precision, Recall, FalseNegatives, FalsePositives\n",
        "results = model.evaluate([input_ids_before_test, input_masks_before_test, input_segments_before_test,\n",
        "                          input_ids_after_test, input_masks_after_test, input_segments_after_test], y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27/27 [==============================] - 6s 225ms/step - loss: 0.4859 - precision: 0.9764 - recall: 0.4820 - false_negatives: 445.0000 - false_positives: 10.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HO273LCLb8J_",
        "outputId": "7c7c709b-5248-4196-ba16-932ff75d2f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4858801066875458, 0.9764150977134705, 0.48195576667785645, 445.0, 10.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0daZhJfQnl7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b5444e9-7ac5-49be-f1c4-3cb9a5b19370"
      },
      "source": [
        "f1_score = 2*((results[1] * results[2])/(results[1] + results[2]))\n",
        "f1_score"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6453624362699225"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}